import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin

def download_images_from_page(url, visited):
    if url in visited:
        return 

    visited.add(url)  
    print(f"Загружаем страницу: {url}")

    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            os.makedirs('images', exist_ok=True)

            images = soup.find_all('img')
            for img in images:
                img_url = img.get('src')
                if img_url:
                    img_url = urljoin(url, img_url)  
                    img_name = os.path.join('images', img_url.split('/')[-1])

                    img_response = requests.get(img_url)
                    if img_response.status_code == 200:
                        with open(img_name, 'wb') as f:
                            f.write(img_response.content)
                        print(f"Сохранено: {img_name}")
                    else:
                        print(f"Ошибка при загрузке изображения: {img_url} (статус {img_response.status_code})")

            links = soup.find_all('a', href=True)
            for link in links:
                link_url = link['href']
                link_url = urljoin(url, link_url) 
                download_images_from_page(link_url, visited) 

        else:
            print(f"Ошибка при загрузке страницы: {response.status_code}")

    except requests.exceptions.RequestException as e:
        print(f"Произошла ошибка: {e}")

start_url = "http://rokot.ibst.psu/anatoly/"
visited_urls = set()  
download_images_from_page(start_url, visited_urls)
